{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42c07ff8-434b-4d5e-a646-53fe2f21caaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Installing packages and libraries to set up environment. Had some problems installing certain packages, especially xarray, so doing so one by one for the most part to make sure they install successfully, and can troubleshoot specific libraries if I run into problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe39746f-3d56-497d-bb52-808c8c7e54b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting numpy==1.24.4\n  Downloading numpy-1.24.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-15f369a0-800c-49da-967e-bc2e05bb9fd5\n    Can't uninstall 'numpy'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nscipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.4 which is incompatible.\nSuccessfully installed numpy-1.24.4\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy==1.24.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938c3bcf-6f6e-4839-8d44-1619a5253a9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting xarray==2023.12.0\n  Downloading xarray-2023.12.0-py3-none-any.whl (1.1 MB)\nInstalling collected packages: xarray\nSuccessfully installed xarray-2023.12.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install xarray==2023.12.0 --no-deps\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e830195f-2d35-488a-94a8-13fa7f5cda4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting s3fs==2023.6.0\n  Downloading s3fs-2023.6.0-py3-none-any.whl (28 kB)\nCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n  Downloading aiohttp-3.11.17-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting aiobotocore~=2.5.0\n  Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\nCollecting fsspec==2023.6.0\n  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\nCollecting botocore<1.31.18,>=1.31.17\n  Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\nCollecting wrapt<2.0.0,>=1.10.10\n  Downloading wrapt-1.17.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\nCollecting aioitertools<1.0.0,>=0.5.1\n  Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (21.4.0)\nCollecting yarl<2.0,>=1.17.0\n  Downloading yarl-1.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (335 kB)\nCollecting async-timeout<6.0,>=4.0\n  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nCollecting aiohappyeyeballs>=2.3.0\n  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nCollecting propcache>=0.2.0\n  Downloading propcache-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\nRequirement already satisfied: typing_extensions>=4.0 in /databricks/python3/lib/python3.9/site-packages (from aioitertools<1.0.0,>=0.5.1->aiobotocore~=2.5.0->s3fs==2023.6.0) (4.1.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.9/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (2.8.2)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /databricks/python3/lib/python3.9/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (1.26.9)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.9/site-packages (from botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (0.10.0)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.31.18,>=1.31.17->aiobotocore~=2.5.0->s3fs==2023.6.0) (1.16.0)\nRequirement already satisfied: idna>=2.0 in /databricks/python3/lib/python3.9/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2023.6.0) (3.3)\nInstalling collected packages: propcache, multidict, frozenlist, yarl, async-timeout, aiosignal, aiohappyeyeballs, wrapt, botocore, aioitertools, aiohttp, fsspec, aiobotocore, s3fs\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.24.32\n    Not uninstalling botocore at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-15f369a0-800c-49da-967e-bc2e05bb9fd5\n    Can't uninstall 'botocore'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nboto3 1.21.32 requires botocore<1.25.0,>=1.24.32, but you have botocore 1.31.17 which is incompatible.\nSuccessfully installed aiobotocore-2.5.4 aiohappyeyeballs-2.6.1 aiohttp-3.11.17 aioitertools-0.12.0 aiosignal-1.3.2 async-timeout-5.0.1 botocore-1.31.17 frozenlist-1.6.0 fsspec-2023.6.0 multidict-6.4.3 propcache-0.3.1 s3fs-2023.6.0 wrapt-1.17.2 yarl-1.20.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install s3fs==2023.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea1aa21-95b3-44c5-8a02-8d3ff2bdf46c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting h5netcdf\n  Downloading h5netcdf-1.6.1-py3-none-any.whl (49 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from h5netcdf) (21.3)\nCollecting h5py\n  Downloading h5py-3.13.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\nRequirement already satisfied: numpy>=1.19.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-15f369a0-800c-49da-967e-bc2e05bb9fd5/lib/python3.9/site-packages (from h5py->h5netcdf) (1.24.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->h5netcdf) (3.0.4)\nInstalling collected packages: h5py, h5netcdf\nSuccessfully installed h5netcdf-1.6.1 h5py-3.13.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install h5netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3061d0e-a8cd-4616-8bea-7c6dd98e6cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting cftime\n  Downloading cftime-1.6.4.post1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nRequirement already satisfied: numpy>1.13.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-15f369a0-800c-49da-967e-bc2e05bb9fd5/lib/python3.9/site-packages (from cftime) (1.24.4)\nInstalling collected packages: cftime\nSuccessfully installed cftime-1.6.4.post1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install cftime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "284daf47-a842-4dee-ab1e-0fc8a300e5aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ce30c8-7da6-446a-b5a1-636c3ceae474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n2023.12.0\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import s3fs\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(np.__version__)\n",
    "print(xr.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa1e6d6d-e25b-4f98-b54a-84457025cfa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Setting some parameters. The s3 bucket name. the start and end date for the files that have been pulled into the s3 bucket. Then creating an array of the dates for use in our for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf12ec75-6ea4-4cd9-8749-a253f85d96c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20240118', '20240119', '20240120', '20240121', '20240122', '20240123', '20240124', '20240125', '20240126', '20240127', '20240128', '20240129', '20240130', '20240131']\n"
     ]
    }
   ],
   "source": [
    "# S3 config\n",
    "bucket = \"s3-dle-930613e9-20bb-49c5-a6ae-0c9aab0ecba3\" # copy over the name of the s3 bucket that you created and moved the files to.\n",
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# Date range\n",
    "start = datetime.strptime(\"20240101\", \"%Y%m%d\") #20240101 - cluster terminated after saving file 20240126\n",
    "end = datetime.strptime(\"20240131\", \"%Y%m%d\")\n",
    "dates = [(start + timedelta(days=i)).strftime(\"%Y%m%d\") for i in range((end - start).days + 1)]\n",
    "\n",
    "print(dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7d471af-6c6a-42ae-9c52-3aea4e86090c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Setting the location of where we are putting the files we pull in from the s3 bucket. The four loop opens the files by data, then filters the contents of the files to only select data that falls within certain latitudes and longitudes that encompass Minnesota, as well as parts of Minnesota's neighbors. Files are saved as parquet files and the log of successfully saved files, failures, and missing files will appear in the output. This is very beneficial as my cluster typically would time out before all 31 files where read in and saved. In those cases I would go back to the section above, change the dates to only pull in the files that did not process, then run this section again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcfdaa5b-cd4e-44d7-b173-1ed483824810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing 20240118...\nSaved /dbfs/20240118.parquet\n Processing 20240119...\nSaved /dbfs/20240119.parquet\n Processing 20240120...\nSaved /dbfs/20240120.parquet\n Processing 20240121...\nSaved /dbfs/20240121.parquet\n Processing 20240122...\nSaved /dbfs/20240122.parquet\n Processing 20240123...\nSaved /dbfs/20240123.parquet\n Processing 20240124...\nSaved /dbfs/20240124.parquet\n Processing 20240125...\nSaved /dbfs/20240125.parquet\n Processing 20240126...\nSaved /dbfs/20240126.parquet\n Processing 20240127...\nSaved /dbfs/20240127.parquet\n Processing 20240128...\nSaved /dbfs/20240128.parquet\n Processing 20240129...\nSaved /dbfs/20240129.parquet\n Processing 20240130...\nSaved /dbfs/20240130.parquet\n Processing 20240131...\nSaved /dbfs/20240131.parquet\n"
     ]
    }
   ],
   "source": [
    "# Going to filter the data to Minnesota\n",
    "output_dir = \"/dbfs/\" \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for date in dates:\n",
    "    path = f\"{bucket}/{date}.gfs.t12z.sfcf001.nc\"\n",
    "    print(f\" Processing {date}...\")\n",
    "\n",
    "    try:\n",
    "        if s3.exists(path):\n",
    "            with s3.open(path, mode=\"rb\") as f:\n",
    "                ds = xr.open_dataset(f, decode_times=False)  # skip time decode for speed\n",
    "                subset = ds.where(\n",
    "                    (ds.lat >= 43.5) & (ds.lat <= 49.5) &\n",
    "                    (ds.lon >= 262.5) & (ds.lon <= 270.5), # longitude is in positive only\n",
    "                    drop=True\n",
    "                )\n",
    "\n",
    "                # Convert to Pandas then Spark DataFrame\n",
    "                pandas_df = subset.to_dataframe().reset_index()\n",
    "                spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "                # Save each day's subset as Parquet\n",
    "                output_path = f\"{output_dir}{date}.parquet\"\n",
    "                spark_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "                print(f\"Saved {output_path}\")\n",
    "        else:\n",
    "            print(f\"File not found: {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {date}: {e}\")\n",
    "\n",
    "    time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3674f8fd-14aa-4fea-a243-0ac622aaf10e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once all files are run successfully, you can move to the NOAA Data Transformation notebook."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1250669060302903,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "NOAA File Lat and Lon Filtering",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}